{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPHd6MWmKiYpbTvelCIIZZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Hesham/Calestiniks-Gym/blob/main/Copy_of_tries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "from isab_pytorch import ISAB\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def batched_index_select(values, indices):\n",
        "    last_dim = values.shape[-1]\n",
        "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n",
        "\n",
        "# helper classes\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.net(x)\n",
        "\n",
        "# adjacent attention class\n",
        "\n",
        "class AdjacentAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.null_k = nn.Parameter(torch.randn(heads, dim_head))\n",
        "        self.null_v = nn.Parameter(torch.randn(heads, dim_head))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        adj_kv_indices,\n",
        "        mask\n",
        "    ):\n",
        "        b, n, d, h = *x.shape, self.heads\n",
        "        flat_indices = repeat(adj_kv_indices, 'b n a -> (b h) (n a)', h = h)\n",
        "\n",
        "        # derive query, key, value\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "\n",
        "        # gather keys and values according to adjacency matrix\n",
        "        k, v = map(lambda t: rearrange(t, 'b h n d -> (b h) n d'), (k, v))\n",
        "        k = batched_index_select(k, flat_indices)\n",
        "        v = batched_index_select(v, flat_indices)\n",
        "        k, v = map(lambda t: rearrange(t, '(b h) (n a) d -> b h n a d', h = h, n = n), (k, v))\n",
        "\n",
        "        # add null key / value, so a node can attend to nothing\n",
        "        # have come across this in GNN literature as some other name\n",
        "        nk, nv = map(lambda t: rearrange(t, 'h d -> () h () () d').expand(b, -1, n, 1, -1), (self.null_k, self.null_v))\n",
        "        k = torch.cat((nk, k), dim = -2)\n",
        "        v = torch.cat((nv, v), dim = -2)\n",
        "        mask = F.pad(mask, (1, 0), value = 1)\n",
        "\n",
        "        # similarity of each node to its neighbors\n",
        "        sim = einsum('b h n d, b h n a d -> b h n a', q, k) * self.scale\n",
        "\n",
        "        # mask out neighbors that are just padding\n",
        "        mask_value = -torch.finfo(sim.dtype).max\n",
        "        mask = rearrange(mask.bool(), 'b n a -> b () n a')\n",
        "        sim.masked_fill_(~mask.bool(), mask_value)\n",
        "\n",
        "        # attention\n",
        "        attn = sim.softmax(dim = -1)\n",
        "\n",
        "        # dropout\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # get weighted average of the values of all neighbors\n",
        "        out = einsum('b h n a, b h n a d -> b h n d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        # combine output\n",
        "        return self.to_out(out)\n",
        "\n",
        "# adjacent network (layers of adjacent attention)\n",
        "\n",
        "class AdjacentAttentionNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        depth,\n",
        "        dim_head = 64,\n",
        "        heads = 4,\n",
        "        num_neighbors_cutoff = None,\n",
        "        num_global_nodes = 0,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_neighbors_cutoff = num_neighbors_cutoff\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            global_attn = PreNorm(dim, ISAB(\n",
        "                dim = dim,\n",
        "                heads = heads,\n",
        "                num_induced_points = num_global_nodes\n",
        "            )) if num_global_nodes > 0 else None\n",
        "\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Residual(PreNorm(dim, AdjacentAttention(\n",
        "                    dim = dim,\n",
        "                    dim_head = dim_head,\n",
        "                    heads = heads,\n",
        "                    dropout = attn_dropout\n",
        "                ))),\n",
        "                global_attn,\n",
        "                Residual(PreNorm(dim, FeedForward(\n",
        "                    dim = dim,\n",
        "                    dropout = ff_dropout\n",
        "                )))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, adjacency_mat, mask = None):\n",
        "        device, n = x.device, x.shape[1]\n",
        "\n",
        "        diag = torch.eye(adjacency_mat.shape[-1], device = device).bool()\n",
        "        adjacency_mat |= diag # nodes should pay attention itself (self-interacting)\n",
        "\n",
        "        # zero out points on adjacency matrix\n",
        "        # where the nodes are just padding\n",
        "        if exists(mask):\n",
        "            adjacency_mat &= (mask[:, :, None] * mask[:, None, :])\n",
        "\n",
        "        adj_mat = adjacency_mat.float()\n",
        "\n",
        "        # if we don't set a hard limit to the number of neighbors:\n",
        "        #   - get the maximum number of neighbors and pad the rest of the nodes with less than that number of neighbors\n",
        "        # else:\n",
        "        #   - randomly sample the cutoff number of neighbors for any node that exceeds the max\n",
        "        #   - this would be similar to random sparse attention (bigbird)\n",
        "\n",
        "        # get the maximum number of neighbors\n",
        "        max_neighbors = int(adj_mat.sum(dim = -1).max())\n",
        "\n",
        "        if exists(self.num_neighbors_cutoff) and max_neighbors > self.num_neighbors_cutoff:\n",
        "            # to randomly sample the neighbors, add a small uniform noise to the mask and topk\n",
        "            noise = torch.empty((n, n), device = device).uniform_(-0.01, 0.01)\n",
        "            adj_mat = adj_mat + noise\n",
        "\n",
        "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = self.num_neighbors_cutoff)\n",
        "\n",
        "            # cast the mask back to 0s and 1s\n",
        "            adj_mask = (adj_mask > 0.5).float()\n",
        "        else:\n",
        "            # todo - get distribution of number of neighbors, and strategically break up attention (message passing) to multiple steps\n",
        "            #      - start with a bimodal num neighbors test case, then generalize\n",
        "\n",
        "            # use topk to get all the neighbors\n",
        "            # also pass the mask into the attention, as some neighbors will be just padding and not actually neighbors\n",
        "            adj_mask, adj_kv_indices = adj_mat.topk(dim = -1, k = max_neighbors)\n",
        "\n",
        "\n",
        "        for attn, global_attn, ff in self.layers:\n",
        "            x = attn(\n",
        "                x,\n",
        "                adj_kv_indices = adj_kv_indices,\n",
        "                mask = adj_mask\n",
        "            )\n",
        "\n",
        "            if exists(global_attn):\n",
        "                out, _ = global_attn(x, mask = mask)\n",
        "                x = x + out\n",
        "\n",
        "            x = ff(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "FSNnLbVvty6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### embedding\n"
      ],
      "metadata": {
        "id": "R3wKp0el1RoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 max_seq_len,\n",
        "                 dim_m,\n",
        "                 vocab_size,\n",
        "                 emb_size=None,\n",
        "                 embeddings=None):\n",
        "        \"\"\"Embeddings with positional encoding.\n",
        "\n",
        "        Args:\n",
        "            max_seq_len (int): Max length of the sequence.\n",
        "            dim_m (int): Model dimension.\n",
        "            vocab_size (int): Vocabulary size.\n",
        "            emb_size (int, optional): Embedding size. You do not need to specify a value if you are using\n",
        "              embedding weights.\n",
        "            embeddings (torch.Tensor, optional): Tensor `(vocab_size, emb_size)` of embeddings weights. Embedding size\n",
        "              value would inherited from shape of this tensor.\n",
        "\n",
        "        Inputs:\n",
        "            - **input**: Long tensor of shape `(batch, seq_len)` - input sequence.\n",
        "\n",
        "        Outputs:\n",
        "            - **output**: Float tensor of shape `(batch, seq_len, emb_size)` - output sequence.\n",
        "\n",
        "        Notes:\n",
        "            - Model dimension and embedding size haven't to be equal. There is an alignment layer, that project\n",
        "              embedding to model size.\n",
        "        \"\"\"\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.max_seq_len = max_seq_len + 1\n",
        "        self.dim_m = dim_m\n",
        "\n",
        "        self.positional = nn.Embedding(max_seq_len + 1, dim_m, padding_idx=0)\n",
        "\n",
        "        if embeddings is None:\n",
        "            self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        else:\n",
        "            emb_size = embeddings.shape[1]\n",
        "            self.embedding = nn.Embedding(*embeddings.shape)\n",
        "            self.embedding.weight = nn.Parameter(\n",
        "                embeddings, requires_grad=False)\n",
        "\n",
        "        self.alignment = nn.Linear(emb_size, dim_m, bias=False)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def forward(self, input):\n",
        "        mask = input == 0\n",
        "\n",
        "        pos_mask = self.position_mask(input)\n",
        "        pos_mask.masked_fill_(mask, 0)\n",
        "        return self.alignment(\n",
        "            self.embedding(input)) + self.positional(pos_mask)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Lookup table for position codes: (max_seq_len, dim_m)\n",
        "        weights = [\n",
        "            self.sin_position_scale(i, np.arange(0, self.dim_m))\n",
        "            for i in range(self.max_seq_len)\n",
        "        ]\n",
        "        weights = np.stack(weights)\n",
        "        weights[1:, ::2] = np.sin(weights[1:, ::2])\n",
        "        weights[1:, 1::2] = np.cos(weights[1:, 1::2])\n",
        "        self.positional.weight = nn.Parameter(\n",
        "            torch.tensor(weights, dtype=torch.float), requires_grad=False)\n",
        "\n",
        "    def sin_position_scale(self, pos, i):\n",
        "        \"\"\"Position scaling :math:`pos/10000^{i*dim_m}` for Sinusoidal Positional Encoding.\n",
        "\n",
        "        Args:\n",
        "            pos (int): Position index.\n",
        "            i (numpy.ndarray): Dimension indexes.\n",
        "\n",
        "        Returns:\n",
        "            float: Scaled value.\n",
        "        \"\"\"\n",
        "        return pos / np.power(1e4, i / self.dim_m)\n",
        "\n",
        "    @staticmethod\n",
        "    def position_mask(tensor):\n",
        "        \"\"\"Generate position mask for tensor.\n",
        "\n",
        "        Args:\n",
        "            tensor (torch.tensor): a float tensor of shape `(batch_size, seq_len, *)`.\n",
        "\n",
        "        Returns:\n",
        "            torch.tensor: an int tensor of word positions.\n",
        "\n",
        "        \"\"\"\n",
        "        # Maybe it would be more productive to use a global buffer of positions `(max_batch_size, max_seq_len)`\n",
        "        # and get a mask from this buffer using slicing.\n",
        "        batch_size, seq_len = tensor.shape\n",
        "        mask = torch.arange(\n",
        "            1, seq_len + 1, dtype=torch.long, device=tensor.device).repeat(\n",
        "                batch_size, 1)\n",
        "\n",
        "        return mask"
      ],
      "metadata": {
        "id": "6Zwc50KD1W9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### position_wise\n"
      ],
      "metadata": {
        "id": "YNs-vayq1X6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionWise(nn.Module):\n",
        "    def __init__(self, dim_m, dim_i, dropout=0.1):\n",
        "        \"\"\"Position-wise Feed-Forward Network.\n",
        "\n",
        "        Args:\n",
        "            dim_m (int): input and output dimension.\n",
        "            dim_i (int): inner dimension.\n",
        "            dropout (float, optional): dropout probability.\n",
        "\n",
        "        Inputs:\n",
        "            - **input** of shape `(batch, *, dim_m)`: a float tensor.\n",
        "\n",
        "        Outputs:\n",
        "            - **output** of shape `(batch, *, dim_m)`: a float tensor.\n",
        "        \"\"\"\n",
        "        super(PositionWise, self).__init__()\n",
        "\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(dim_m, dim_i), nn.ReLU(), nn.Linear(dim_i, dim_m),\n",
        "            nn.Dropout(dropout))\n",
        "        self.normalization = nn.LayerNorm(dim_m, eps=1e-12)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # There's nothing difficult here.\n",
        "        residual = input\n",
        "        output = self.feedforward(input)\n",
        "        output = self.normalization(output + residual)\n",
        "        return output"
      ],
      "metadata": {
        "id": "E_nM-CdS1crw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformer"
      ],
      "metadata": {
        "id": "fq4gDvnp1dez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, dim_m, dim_q_k, dim_v, n_heads, dim_i, dropout):\n",
        "        \"\"\"Transformer encoder layer.\n",
        "\n",
        "        Args:\n",
        "            dim_m (int): Dimension of model.\n",
        "            dim_q_k (int): Dimension of `query` & `key` attention projections.\n",
        "            dim_v (int): Dimension of `value` attention projection.\n",
        "            n_heads (int): Number of attention heads.\n",
        "            dim_i (int): Inner dimension of feed-forward position-wise sublayer.\n",
        "            dropout (float): Dropout probability.\n",
        "\n",
        "        Inputs:\n",
        "            - **input** of shape `(batch, enc_seq_len, dim_m)`, a float tensor, where `batch` is batch size,\n",
        "              `enc_seq_len` is length of encoder sequence for this batch and `dim_m` is hidden size of model.\n",
        "              Input embedding has `dim_m` size too.\n",
        "\n",
        "        Outputs:\n",
        "            - **output** of shape `(batch, seq_len, dim_m)`, a float tensor.\n",
        "        \"\"\"\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "        self.attention = AdjacentAttentionNetwork(n_heads, dim_m, dim_q_k, dim_v,\n",
        "                                            dropout)\n",
        "        self.positionwise = PositionWise(dim_m, dim_i, dropout)\n",
        "\n",
        "    def forward(self, input, mask=None,return_attn_weight=False):\n",
        "        if(return_attn_weight):\n",
        "            enc_att,attn_weight=self.attention(input, input, input, mask=mask,return_attn_weight=True)\n",
        "            output = self.positionwise(enc_att)\n",
        "            return output,attn_weight\n",
        "        else:\n",
        "            enc_att = self.attention(input, input, input, mask=mask,return_attn_weight=False)\n",
        "            output = self.positionwise(enc_att)\n",
        "            return output\n",
        "\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, dim_m, dim_q_k, dim_v, n_heads, dim_i, dropout):\n",
        "        \"\"\"Transformer decoder layer.\n",
        "\n",
        "        Args:\n",
        "            dim_m (int): Dimension of model.\n",
        "            dim_q_k (int): Dimension of `query` & `key` attention projections.\n",
        "            dim_v (int): Dimension of `value` attention projection.\n",
        "            n_heads (int): Number of attention heads.\n",
        "            dim_i (int): Inner dimension of feed-forward position-wise sublayer.\n",
        "            dropout (float): Dropout probability.\n",
        "\n",
        "        Inputs:\n",
        "            - **input** of shape `(batch, dec_seq_len, dim_m)`, a float tensor, where `batch` is batch size,\n",
        "              `dec_seq_len` is length of decoder sequence for this batch and `dim_m` is hidden size of model.\n",
        "              Input embedding has `dim_m` size too.\n",
        "            - **encoder_output** of shape `(batch, enc_seq_len, dim_m)`, a float tensor, where `enc_seq_len` is length\n",
        "              of encoder sequence.\n",
        "            - **mask** of shape `(batch, dec_seq_len, dec_sec_len)`, a byte tensor containing mask for\n",
        "              illegal connections between encoder and decoder sequence tokens. It's used to preserving\n",
        "              the auto-regressive property.\n",
        "\n",
        "        Outputs:\n",
        "            - **output** of shape `(batch, dec_seq_len, dim_m)`, a float tensor.\n",
        "        \"\"\"\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "\n",
        "        self.masked_attention = AdjacentAttentionNetwork(n_heads, dim_m, dim_q_k,\n",
        "                                                   dim_v, dropout)\n",
        "        self.attention = AdjacentAttentionNetwork(n_heads, dim_m, dim_q_k, dim_v,\n",
        "                                            dropout)\n",
        "        self.positionwise = PositionWise(dim_m, dim_i, dropout)\n",
        "\n",
        "    def forward(self, input, encoder_output, mask, extra_mask=None):\n",
        "        dec_att = self.masked_attention(input, input, input, mask)\n",
        "        adj_att = self.attention(\n",
        "            value=encoder_output, key=encoder_output, query=dec_att,mask=extra_mask)\n",
        "        output = self.positionwise(adj_att)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 max_seq_len,\n",
        "                 vocab_size,\n",
        "                 emb_size=250,\n",
        "                 embeddings=None,\n",
        "                 n_layers=6,\n",
        "                 dim_m=512,\n",
        "                 dim_q_k=64,\n",
        "                 dim_v=64,\n",
        "                 n_heads=8,\n",
        "                 dim_i=2048,\n",
        "                 dropout=0.1):\n",
        "        \"\"\"Transformer model from 'Attention Is All You Need' paper.\n",
        "\n",
        "        Args:\n",
        "            max_seq_len (int): Maximum sequence length.\n",
        "            vocab_size (int): Vocabulary size.\n",
        "            emb_size (int, optional): Embedding size. You do not need to specify a value if you are using\n",
        "              embedding weights.\n",
        "            embeddings (torch.Tensor, optional): Long tensor of shape `(vocab_size, emb_size)` - embedding tensor.\n",
        "              Embedding size value would inherited from shape of this tensor.\n",
        "            n_layers (int, optional): Number of transformer layers.\n",
        "            dim_m (int, optional): Model hidden size, must be equal with embedding size.\n",
        "            dim_q_k (int, optional): Dimension of `query` & `key` attention projections.\n",
        "            dim_v (int, optional): Dimension of `value` attention projection.\n",
        "            n_heads (int, optional): Number of attention heads.\n",
        "            dim_i (int, optional): Inner dimension of feed-forward position-wise sublayer.\n",
        "            dropout (float, optional): Dropout probability.\n",
        "\n",
        "        Variables:\n",
        "            - **encoder_state**: a float tensor of shape `(batch, enc_seq_len, dim_m)` containing encoder state from\n",
        "              last layer.\n",
        "\n",
        "        Inputs:\n",
        "            - **enc_seq** of shape `(batch, enc_seq_len)`, a long tensor encoder input sequence.\n",
        "            - **dec_seq** of shape `(batch, dec_seq_len)`, a long tensor decoder input sequence.\n",
        "\n",
        "        Outputs:\n",
        "            - **output** of of shape `(batch, dec_seq_len, vocab_size)`, a float tensor of vocabulary probability\n",
        "              distribution.\n",
        "\n",
        "        Notes:\n",
        "            - For optimizing model, encoder state stores in local variable and calculate only one per batch. After\n",
        "              auto-regressive process encoder state must be reset. You can do this using\n",
        "              :func:`Transformer.reset_encoder_state`.\n",
        "        \"\"\"\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.positional_encoding = PositionalEmbedding(\n",
        "            max_seq_len, dim_m, vocab_size, emb_size, embeddings)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(dim_m, dim_q_k, dim_v, n_heads, dim_i,\n",
        "                                    dropout) for i in range(n_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(dim_m, dim_q_k, dim_v, n_heads, dim_i,\n",
        "                                    dropout) for i in range(n_layers)\n",
        "        ])\n",
        "        # I think it's better to use smooth transition from dim_m to vocab_size\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(dim_m, vocab_size),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Linear(7000, vocab_size),\n",
        "        )\n",
        "        self.softmax = nn.Softmax(-1)\n",
        "\n",
        "        self.encoder_state = None\n",
        "\n",
        "    def forward(self, enc_seq, dec_seq):\n",
        "        # Calculate encoder state for batch.\n",
        "        if self.encoder_state is None:\n",
        "            # Sum embeddings with positional encodings.\n",
        "            self.encoder_state = self.positional_encoding(enc_seq)\n",
        "\n",
        "            for enc_layer in self.encoder_layers:\n",
        "                self.encoder_state = enc_layer(self.encoder_state)\n",
        "\n",
        "        # Decoder block.\n",
        "        # Apply positional encoding.\n",
        "        dec_state = self.positional_encoding(dec_seq)\n",
        "\n",
        "        mask = self.autoregressive_mask(dec_seq)\n",
        "\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_state = dec_layer(dec_state, self.encoder_state, mask)\n",
        "\n",
        "        output = self.out(dec_state)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def reset_encoder_state(self):\n",
        "        \"\"\"Reset previous encoder state of batch. This method must calls before process new batch.\n",
        "        \"\"\"\n",
        "        self.encoder_state = None\n",
        "\n",
        "    @staticmethod\n",
        "    def autoregressive_mask(tensor):\n",
        "        \"\"\"Generate auto-regressive mask for tensor. It's used to preserving the auto-regressive property.\n",
        "\n",
        "        Args:\n",
        "            tensor (torch.Tensor): of shape `(batch, seq_len, dim)`.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: a byte mask tensor of shape `(batch, seq_len, seq_len)` containing mask for\n",
        "            illegal attention connections between decoder sequence tokens.\n",
        "\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = tensor.shape\n",
        "        x = torch.ones(\n",
        "            seq_len, seq_len, device=tensor.device).tril(-1).transpose(0, 1)\n",
        "\n",
        "        return x.repeat(batch_size, 1, 1).byte()"
      ],
      "metadata": {
        "id": "cCFFNlQ01fAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gLGE-Ce71qT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}